# Machine Learning Final Project - gkuch22 lsurm22

  

## პროექტის სათაური:

Walmart Recruiting - Store Sales Forecasting

კეგლის ამ ჩელენჯში გვევალებოდა რომ გამოგვეკვლია როგორი გავლენა აქვს სხვადასხვა მახასიათებლებს ვოლმარტის მაღაზიის გაყიდვებში და შემდეგ გვეწინასწარმეტყველებინა ახლო მომავალი.

  

## რეპოზიტორიის სტრუქტურა:

**final-model-experiment-{model_name}** - თითოეულ ასეთო ფაილში აღწერილია ერთი კონკრეტული არქიტექტურა.

**final-model-inference** - ამ ფაილში ხდება ყველა მოდელს შორის საუკეთესოს mlflow -დან ჩამოტვირთვა და საბოლოო submission-ის კეგლის competition-სთვის დაგენერირება.

**README.md** - პროცესის დეტალური აღწერა.

  
  
  

## Training:

  

## XGBoost

გადავწყვიტე ნაცნობი მოდელით დამეწყო ამ ჩელენჯის დამუშავება. პირველ გაშვებაზე უბრალოდ ავიღე train.csv მხოლოდ სხვა მონაცემებთან გაერთიანების გარეშე, დავსორტე დატა დროის მიხედვით და ისე გავყავი train valid test სეტებად და default ჰიპერპარამეტრებით გავუშვი xgboost რომ მენახა რა შედეგი დაიდებოდა weighted mae ზე. 8600 8100 არის საწყისი წერტილი მოდელის გაუმჯობესებამდე. შემდეგ ჯერზე გადავწყვიტე სხვა მონაცმეებიც გამომეყენებინა. ამიტო stores და features სეტები დავმერჯე train თან. აქ features ში დროის მხრივ უფრო გვიანდელი მონაცემები გვაქვს 2013 წლის რაც train ში არ ხდება თუმცა ამას შემდეგ მივხედავთ, ჯერჯერობით left merge ამ ნაწილს არ გადააბამს. შედეგად მივიღე 15 სვეტიანი თეიბლი. კატეგორიული სვეტები მარტო type გვხვდება ამიტო ჯერჯერობით OrdinalEncoder ით გადავაკეთე რიცხვებში. ხოლო Nan ველიეუები SimpleImputer ის გამოყენებით mean ით შევავსე. ასევე Date სვეტი გავხლიჩე და year month week სამი ახალი ფიჩერი მივიღე. საწყისი Date ცვლადის სხვა რაიმე რიცხით წარმოდგენას ასე დაყოფა ვამჯობინე, რადგან ვფიქრობ უკეთ მიხვდება მოდელი დროის მხრივ sales ის მიხვედრას. რაც შეეხება სპლიტს, დატა როგორც ვთქვი დავსორტე და იმისთვის რომ რენდომად არ გადალაგებულიყო როუები, year_split month_split threshold ები შემოვიღე რის მიხედვითაც დავყავი დატა სამ ნაწილად. ჯერჯერობით 2012 წლამდე მონაცემები train ში გავანაწილე და შემდეგ 2012 ის მონაცმები თანაბრად გავყავი valid და test სეტებში. xgboost ის ჰიპერპარამეტრები იგივე დავტოვე წინა გაშვებიდან, შედეგი გაუმჯობესდა 7290 6990 ერორმა დაახლოებით 1200 ით დაიკლო წინა გაშვებიდან, ანუ features და stores დატასეტები მნიშვნელოვან ინფორმაციას შეიცვადნენ მოდელზე დასახმრებლად. ასევე დავპლოტე feature importance ები და ვხედავ, რომ dept, size, store ს ყველაზე დიდი მნიშვნელობა ენიჭება მოდელისგან. შემდეგ გაშვებაზე type-ს onehot encoder ით გარდავქმნი ორდინალის მაგივრად, ასევე საშუალოდ სეილების ავარდნაზე დაკვირვებით დავინახე რომ ახალწლამდე დიდი სპაიკები გვაქვს, ამიტო გადავწყვიტე დატაში WeeksToNewYear სვეტი ჩამემატებინა, რომელიც 1 2 3 კვირით ადრე კვირებს მოუნიშნავს მოდელს და ასევე დროის აღქმას გაუუმჯობესებს წესით. 7135 6833 შედეგი გაუმჯობესდა. weekstonewyear სვეტს ნორმალური importance მიენიჭა. ამის შემდეგ დავაკვირდი რომ holiday ს დროს თითოეული store სხვადასხვანაირად რეაგირებდა sales-ს მხრივ. ამიტომ დავამატე StoreHolidayLift სვეტი, ამით მოდელს ვეხმარები რომ უბრალოდ isHoliday true ან false ის მაგრივრად დამატებით წარმოდგენა შეექმნას საშუალოდ როგორი ავარდნა ხდება holiday ს დროს თითოეულ store ში. ამან შედეგი ოდნავ გააუარესა 7280 7090 მაგრამ StoreHolidayLift importance ში მეორე ადგილი დაიკავა, ამიტომ ეს საჭირო სვეტად ჩავთვალე და სხვა ფიჩერებთან ერთად უკეთეს შედეგს მოგვცემს წესით ბოლოს. იგივე პრინციპით გადავწყვიტე თითოეულ დეპარტამენტზეც განმესაზღვრა რამდენად ხდება sales ის ცვლილება. დავამატე DeptHolidayLift სვეტი. მივიღე ჯერჯეორბით საუკეთესო შედეგი 6700 6600. საკმაოდ დიდი importance მიენიჭა ამ სვეტს, როგორც ვთქვი StoreHolidayLift-ს სხვა ფიჩერთან ერთად კარგი გავლენა ჰქონდა მოდელზე. ამის შემდეგ ახალ წლამდე კვირების დამატების მსგავსად, რამდენიმე კონკრეტულ store department წყვილს დავაკვირდი, როგორ წესი spike-მდე ხშირად შემხვდა isHoliday=True შემთხვევა, ასევე სპაიკის შემდეგაც იყო ხოლმე ჰოლიდეი, ამიტომ გადავწყვიტე დამემატებინა სვეტები, რომლებიც აღნიშნავენ იყო თუ არა წინა და იმის წინა კვირები ჰოლიდეი და იქნება თუ არა შემდეგი და იმის შემდეგი კვირები ჰოლიდეი. ეს წესით დაახლოებით წარმოდგენას შუქმნის მოდელს როდის უნდა ელოდოს დიდ ვარდნას. მცირე მაგრამ მაინც შედარებით უკეთესი შედეგი მომცა ამან. ამის შემდეგ იგივე ლოგიკით მინდოდა დამემატებინა წინა კვირების შესაბმისი sale-ები რომ დროის მხრივ როგორ ხდება ზრდა-მატება უკეთ ესწავლა მოდელს, მაგრამ რადგან ეს ინფორმაცია ტესტის prediction-ის დროს არ გვექნებოდა, ამიტომ დაახლოებით იგივე იდეა რომ გამომიყენებინა, დატა დაახლოებით წლის მიმართ პერიოდულია, ამიტომ გადავყწვიტე დამემატებინა წინა წლის იგივე კვირის ინფორმაცია, ან თუ ამაზე არ მიგვიწვდებოდა ხელი 2 წლის წინანდელი სეილები, ამან კარგი შედეგი დადო. 5400 3100 სქორი ცოტა უცნაურია, მაგრამ რადგან weighted mae ს ვითვლით შესაძლოა ჰოლიდეიბის წილი ისე გადანაწილდა და სირთულე ტრეინსა და ვალიდაციას შორის რომ ასეთი შედეგი მოგვცა, მითუმეთეს რომ ჯერ მოდელის პარამეტრები არაა დატუნინგებული და საკმაოდ მარტივი მოდელია, რასაც ტრეინის სწავლა როგორც ჩანს უჭირს, ამიტომ შემდეგ გაშვებაზე გავზარდე n_estimators, learning rate, max_depth დავუწიე რეგულაციას და უკვე შედარებით ნორმალური შედეგი მოგვცა ერთმანეთის მიმართაც და ზოგად სურათშიც. კიდევ უფრო კომპლექსური გავხადე მოდელი და 1400 2000 ყველაზე კარგი შედეგი მომცა. ასევე დავპლოტე feature importance ები და ჩემი დამატებული ფიჩერები როგორც ჩანს მნიშვნელოვანი აღმოჩნდა მოდელის უკეთ სამუშაოდ.

  
  

## Prophet

თავიდან უბრალოდ train.csv ავიღე store=1 dept=1 წყვილის ინფრომაცია ამოვიღე დავსორტე და გავსპლიტე სამ ნაწილად. 7500 8500 არის მოდელის საწყისი სქორები. ამის შემდეგ, რადგან prophet ს არ შეუძლია პირდაპირ მთლიან დატაზე დატრენინგება, გადავწყვიტე თითოეული store-dept წყვილი ამეღო ცალცალკე და ისე დამეტრენინგებინა მოდელი, მიღებულ პასუხებს ვკინძავ და ვითვლი საერთო wmae-ს. 3000 3100 საწყისი შედეგია. ამის შემდეგ გადავწყვიტე დამატებით regressor ები დამემატებინა prophet მოდელისთვის, რომ უკეთესად მოახერხოს sale ების მატება-ვარდნის დაჭერა დამატებითი ფიჩერების გამოყენებით. ამიტომ train თან stores და features დატასეტებიც დავმერჯე და პირველ რიგში isHoliday სვეტი დავუმატე პარამეტრად და ისე გავუშვი მოდელი, თუმცა ამას დიდად არ გაუმჯობესებია შედეგი. ამის შემდეგ გადავწყვიტე დანარჩენი ფიჩერებიც დამემატებინა, თუმცა ამოვარჩიე უფრო მნიშნველოვნები, რადგან ყველას ერთიანად დამატება ოვერფიტში წაგვიყვანდა, იმიტორო თითო store-dept წყვილებად რომ ვყოფ ისედაც პატარავდება უფრო დატა, 2700 4200 ტრეინზე სქორი შემცირდა თუმცა ვალიდაციაზე გაიზარდა, ანუ მაინც ოვერფიტისკენ წავიდა მოდელი და ახლა საჭიროა ტუნინგი. გადავწყვიტე changepoint_prior_scale და seasonality_prior_scale შემემცირებინა რომ ოვერფიტი ამერიდებინა თავიდან, 1900 2200 საკმაოდ კარგი შედეგი დაიდო წინასთან შედარებით, ოვეფიტის პრობლემაც შემცირდა. ამის შემდეგ რამდენიმე გაშვება ვცადე მაგრამ უარესი შედეგები მომცა და აღარ დამილოგავს, ასევე seasonality_mode multiplicative იც ვცადე რადგან თითქოს სპაიკები შეესაბამებოდა სქეილინგის შემთხვევას მაგრამ ამანაც გააუარესა შედეგი. ამ მოდელის ტრენინგს აქ შევაჩერებ რადგან ამაზე უფრო კომპლექსური მოდელებიც გვაქვს განსახილი რომლებიც უკეთ მოერგება ჩვენს ამოცანას.

  
  

## Arima

თავიდან აქაც prophet ის მსგავსად ერთი წყვილი store-dept ავიღე და დავატრენინგე მოდელი p,d,q = 1,1,1 პარამეტრებით. 6700 8650. prophet თან შედარებით მეტი ოვერფიტი აჩვენა, სხვა მხრივ ვალიდაციის ქულით ჯერჯერობით ვერ ჯობს მას. ამის შემდეგ რამდენიმე store-dept წყვილი დავპლოტე, განსაკუთრებული ზრდადი ან კლებადი ტრენდი ძირითადად არ შეინიშნება, თუმცა მაინც გვხვდება ზოგგან, ამიტომ გადავწყვიტე d პარამეტრი 0 სა და 1-ს შორის ვცვალო, რომ ვნახო რომელი უკეთესად მოერგება მთლიან დატას. რაც შეეხება p და q პარამეტრებს, სხვადასხვა წყვილისთვის ეს რიცხვები მერყეობს, ამიტომ გადავწყივტე p,d,q = 1,0,1 დან დავიწყო მოდელის დატრენინგება და დაახლოებით 5-8 რეინჯამდე მივიდე და ვნახო შედეგები. საწყისმა 2600 2700 აჩვენა, რაც ნორმალური შედეგია და prophet-ს ჯობია. შემდეგ რანზე როგორც ვთქვი იგივე p და q ორდერები დავტოვე და d=1 ავირჩიე რომ ვნახო ერთი დიფერენსი ხო არ ახდენს უკეთეს გავლენას, თუმცა ამან ტრეინის ორივე სეტის შედეგები გააუარესა და 2900 4000 მომცა. ამის შემდეგ 2,0,2 ვცადე, რამაც ძალიან დიდი ერორი აჩვენა, ვფიქრობ ეს d=0 ის ბრალია, 1,0,1 ის დროს ნორმალური შედეგი აჩვენა, თუმცა შემდეგ როცა მოდელი უფრო კომპლექსური გახდა p და q ს გაზრდით ბოლომდე stationary დატა არ გამოდგა და საჭირო გახდა დიფფ ის გაზრდა, ამიტომ 2,1,2 ვცადე შემდეგ ჯერზე და იმხელა ერორები აღარ გვქონდა თუმცა 2800 3800 წინა რანებთან შედარებით მაინც ცუდი შედეგია. რადგან p და q ის გაზრდით უარესი შედეგები იწყება, გადავწყვიტე მეტად აღარ გავზარდო და 1,1,2 და 2,1,1 ვარიანტები ვცადე, სადაც 2,1,1 მა აჯობა 2900 3700 ქულით, თუმცა უფრ ომარტივ მოდელებთან შედარებით მაინც უარესი შედეგი დადო. გამოდის რომ რადგან store-dept წყვილებად დაყოფა გვაძლევს საკმაოდ პატარა ზომის დატასეტებს მასზე დასატრენინგებლად უკეთესია უფრო მარტივი მოდელები და არ სჭირდება წინა ბევრი შემთხვევის გამოყენება კარგი შედეგის მისაღებად. ამ მოდელზე აქ შევჩერდები, რადგან ორდერ პარამეტრები ამაზე უკეთ წესით აღარ გაუმჯობესდება. ამ მიზეზით ვიფიქრე დატას სხვანაირი გასპლიტვა, store ების მიხედვით რომ დამეჯგუფებინა, თუმცა ერთ სთორში სხვადასხვა დეპარტამენტები საკმარისად არ ჰგავს ერთმანეთს და საშუალოს ამოღება უარეს შედეგს მოგვცემდა. ასევე ვაპირებდი log ან boxcox ის გამოყენებას მთლიან დატაზე, როგორც ეს ანალიზის ნაწილში მაქვს ნაჩვენები, თუმცა სეილები უარყოფით მნიშვნელობებსაც შეიცავს, ამის გასწორება შეიძლებოდა კონტანტის დამატებით და გადაჩოჩებით თუმცა ეს მეტ ერორს გამოიწვევდა ვფიქრობ და აღარ მიცდია.

  
  

## PatchTST

თავიდან გადავწყვიტე უბრალოდ გამეშვა მოდელი რომ მენახა საწყისი ქულა. train.csv ავიღე, unique_id შევუქმენი მოდელს store_dept ის მიდგმით, რომ მოდელმა გაარჩიოს მაღაზიები ერთმანეთისგან, გავსპლიტე დატა და გავუშვი მოდელი. ტრეინის დატასეტი 52 კვირაზე მეტი დროის ინფორმაციას შეიცავს თუმცა ამ მომენტისთვის input_size=52 ანუ ერთი წელი ავიღე მოდელის დასატრენინგებლად, max_steps=200. მივიღე 2400 valid wmae საწყის სქორად. სამწუხაროდ ამ მოდელისთვის train wmae ვერ გავიგე კარგად როგორ დამეთვალა, სხვა ფრეიმვორკებიც ვცადე მაგალითად darts მაგრამ მაინც ვერ შევძელი დემეპრედიქტებინა იმ დატაზე სეილები რომელზეც ტრენინგდება მოდელი, რომ მერე ორი სქორის შედარება შემძლებოდა ოვერფიტი ანდერფიტის გასაგებად. ვიფიქრე ტრეინს გავსპლიტავ მეთქი კიდე თუმცა ეს არაა ლოგიკური იმიტორო ჩვენ train_wmae როცა გვაინტერესებს მთელი იდეა იმაშია რომ ვნახოთ რა შედეგი აქვს მოდელს იმ დატაზე რომელზეც დატრენინგდა. ამიტომ ამ მოდელს მხოლოდ valid wmae თი შევაფასებ. რაც დიდი პრობლემა არ უნდა იყოს იმიტორო train_wmae რომ მქონდეს მაგალითდ მარტო არ ივარგებდა იმიტორო ძალიან ოვერფიტში წასული მოდელი რომ დამეტრენინგებინა შეიძლებოდა სულ გამენულებინა ტრეინის ერორი, თუმცა ამ დროს ვალიდის სქორი საკმაოდ ცუდი იქნებოდა, ახლა კი როცა ვალიდის სქორს ვხედავ, მისი მინიმაზაციაა ჩემი მიზანი და არ იქმენბა დიდი პრობლემა. ამის შემდეგ გაშვებაზე მოამატე max_steps რაოდენობა რომ მეტად შეძლოს უფრო რთული დეტალების დაჭერა მოდელმა და სამაგიეორდ შევამცირე learning rate რომ მეტ ნაბიჯში ნელნელა შეასწოროს კოეფიციენტები და უკეთ მივიდეს კარგ შედეგამდე. 2449 თითქმის იგივე დარჩა სქორი. ამის შემდეგ encoder_layers default 3 დან 4 მდე გავზარდე ისევ იმ მიზნით რომ უკეთ ისწავლოს მოდელმა კომპლექტური კავშირები და ასევე გავზარდე batch_size რომ უფრო დიდ ჩანკებს დახედოს მოდელმა თითო ჯერზე და ვნახავ როგორი შედეგი ექნება. 2425 ოდნავ გაუმჯობესდა მოდელი. ამის შემდეგ რანზე კიდევ უფრო გავზარდე max_steps და ასევე გავზარდე dropout რომ ამდენი პარამეტრების ცვილელების და მოდელის გართულების შემდეგ დიდ ოვერფიტში არ წავიდეს მოდელი და უკეთესად მოახდინეს განზოგადება. დაახლოებით იგივე რეინჯში დარჩა მოდელი. ამის შემდეგ გავზარდე input_size რადგანა ხლა 52 კვირა ანუ 1 წელზეა დასეტილი მარა ტრეინის დატა მეტს მოიცავს და შესაძლოა გამოადგეს მოდელს და კიდე გავზარდე max_steps. 1980 საკმაოდ კარგი შედეგი მივიღე, როგორც ჩანს კარგი გავლენა იქონია წარსულის დატამ, ამიტომ ინპუტ საიზი ორ წლამდე გავზარდე, თუმცა ერორები რო არ გამოწვეულიყო პადინგი დავამატე წარსულის როუებზე რომ არ გადაცდეს კვირების რაოდენობა. მარა ამან 2200 დაწერა ანუ გააუარესა შედეგი ტრეინის სქორს რო ვხედავდე ეხლა ალბათ ოვერფიტისკენ წავიდა მოდელი და ტრეინის სქორის დავარდნას ვნახავდით. ამიტომ ამ მოდელზე აქ შევჩერდები, ვფიქრობ 1980 კარგი შედეგია, მოდელის კვალობაზე რომელიც დამატებით ფიჩერებს არ იყენებს რო დაიხმაროს კავშირები დატაში.

  
  

## TFT

ვიყენებ იგივე ფრეიმვორკს, მიდგომა ჰგავს patchTST-ს. თავიდან გადავწყვიტე აქაც მხოლოდ სამი მთვარი სვეტი unique_id ds y გადამეცა მოდელისთვის რომ მენახა საყიწი სქორი. 2500 valid wmae patchTST ის მსგავსი შედეგი მივიღე. ამის შემდეგ გადავწყვიტე დამატებით ფიჩერები მიმეხმარებინა მოდელისთვის თუმცა ამან კარგი შედეგი ნამდვილად არ გამოიღო, ჯერ IsHoliday დავუმატე მხოლოდ და ოდნავ გააუარესა შედეგი. ამის შემდეგ Temperature და Fuel_Price იც დავამატე და სტეპები შედარბით მოვუმატე რომ გაუმჯობესებეულიყო მოდელი, მარა 5500 მდე ავარდა სქორი, როგორც ჩანს დამატებით ფიჩერები ხელს უშლის ამ მოდელს და პარამეტრების დატუნინგებაზე ვკონცენტრირდები. patchTST ის წინა რანებიდან გამოვიყენე ველიუები აქაც და 2390 wmae მივიღე, გაუმჯობესდა მოდელი. ამის შემდეგ patchTST ზე საუკეთესო შედეგი რამაც მომცა ზუსტად ის ველიეუბი დავსვი იმ განსხვავებით რომ აქ enocde_layers არგუმენტი აღარ გვაქვს, ამან 2630 მომცა. ამიტომ წინა მოდელის პარამეტრებს შევეშვი და ვცდი რომ გავაუმჯობესო მოდელი სწორი პარამეტრების პოვნით. შემდეგ გაშვებაზე შემოვიყვანე ახლაი პარამეტრი n_rnn_layers და დეფოლტ მნიშვნელობიდან გავზარდე რომ უკეთ დაიჭიროს მოდელმა კომპლექსური კავშირები, შედეგად 2421 მივიღე. ამის შემდეგ კიდევ გავზარდე და ასევე გავზარდე დროპაუტი რომ არ წავსულიყავი დიდ ოვერფიტში, შედეგად მივიღე 2555. ვფიქრობ ამ მოდელზე აქ შევჩერდები რადგან იმედი მქონდა რომ patchTST ის შემდეგ ეს მოდელი უკეთესი იქნებოდა იმიტორო თან დამატებით ფიჩერების გამოყენბაც შეუძლია, თუმცა რადგან ამან არ გააუმჯობესა მოდლეი, პარამეტრების ტუნინგით მაქსიმუმ წინა შედეგს თუ მივუახლოვდები მხოლოდ.

  
  
  

## MLFLOW Tracking:

ექსპერიმენტების ბმული: https://dagshub.com/gkuch22/ml-final.mlflow

  

საუკეთესო მოდელის შედეგი: PatchTST. Public Score: 2820.22480 Private score: 2932.98180
