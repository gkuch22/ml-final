{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3816,"databundleVersionId":32105,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-06T12:33:05.344565Z","iopub.execute_input":"2025-07-06T12:33:05.345537Z","iopub.status.idle":"2025-07-06T12:33:05.352107Z","shell.execute_reply.started":"2025-07-06T12:33:05.345505Z","shell.execute_reply":"2025-07-06T12:33:05.351260Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/sampleSubmission.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv\n/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/test.csv.zip\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"train_csv = pd.read_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip')\ntrain_csv.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T12:33:07.030620Z","iopub.execute_input":"2025-07-06T12:33:07.031280Z","iopub.status.idle":"2025-07-06T12:33:07.315964Z","shell.execute_reply.started":"2025-07-06T12:33:07.031253Z","shell.execute_reply":"2025-07-06T12:33:07.314998Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   Store  Dept        Date  Weekly_Sales  IsHoliday\n0      1     1  2010-02-05      24924.50      False\n1      1     1  2010-02-12      46039.49       True\n2      1     1  2010-02-19      41595.55      False\n3      1     1  2010-02-26      19403.54      False\n4      1     1  2010-03-05      21827.90      False","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Store</th>\n      <th>Dept</th>\n      <th>Date</th>\n      <th>Weekly_Sales</th>\n      <th>IsHoliday</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>2010-02-05</td>\n      <td>24924.50</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>2010-02-12</td>\n      <td>46039.49</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1</td>\n      <td>2010-02-19</td>\n      <td>41595.55</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>2010-02-26</td>\n      <td>19403.54</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>1</td>\n      <td>2010-03-05</td>\n      <td>21827.90</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"train_csv['Dept'].unique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T14:27:42.339503Z","iopub.execute_input":"2025-07-05T14:27:42.340128Z","iopub.status.idle":"2025-07-05T14:27:42.347655Z","shell.execute_reply.started":"2025-07-05T14:27:42.340103Z","shell.execute_reply":"2025-07-05T14:27:42.347012Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 16, 17, 18,\n       19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 40, 41, 42, 44, 45, 46, 47, 48, 49, 51, 52, 54, 55, 56,\n       58, 59, 60, 67, 71, 72, 74, 77, 78, 79, 80, 81, 82, 83, 85, 87, 90,\n       91, 92, 93, 94, 95, 96, 97, 98, 99, 39, 50, 43, 65])"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef weighted_mean_absolute_error(y_true, y_pred, weights):\n    \n    weights = np.array(weights)\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    if np.sum(weights) == 0:\n        return np.mean(np.abs(y_true - y_pred))\n    \n    wmae = np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n    return wmae\n\nclass WalmartSARIMAForecaster:\n    def __init__(self):\n        self.models = {}\n        self.forecasts = {}\n        self.holiday_weights = {}\n        \n    def load_and_preprocess_data(self, train_path, test_path, stores_path=None, features_path=None):\n        \n        self.train_data = pd.read_csv(train_path)\n        self.test_data = pd.read_csv(test_path)\n        \n        if stores_path:\n            self.stores_data = pd.read_csv(stores_path)\n        if features_path:\n            self.features_data = pd.read_csv(features_path)\n            self.features_data['Date'] = pd.to_datetime(self.features_data['Date'])\n            self._process_holiday_weights()\n        \n        self.train_data['Date'] = pd.to_datetime(self.train_data['Date'])\n        self.test_data['Date'] = pd.to_datetime(self.test_data['Date'])\n        \n        self.train_data = self.train_data.sort_values(['Store', 'Dept', 'Date'])\n        self.test_data = self.test_data.sort_values(['Store', 'Dept', 'Date'])\n        \n        print(f\"Train data shape: {self.train_data.shape}\")\n        print(f\"Test data shape: {self.test_data.shape}\")\n        \n        self.train_data['Weekly_Sales'] = self.train_data['Weekly_Sales'].fillna(0)\n        \n        self.train_data['Store_Dept'] = self.train_data['Store'].astype(str) + '_' + self.train_data['Dept'].astype(str)\n        self.test_data['Store_Dept'] = self.test_data['Store'].astype(str) + '_' + self.test_data['Dept'].astype(str)\n        \n        self._add_holiday_weights_to_data()\n        \n        return self.train_data, self.test_data\n    \n    def _process_holiday_weights(self):\n        if not hasattr(self, 'features_data'):\n            return\n            \n        for _, row in self.features_data.iterrows():\n            date_key = (row['Store'], row['Date'])\n            is_holiday = row['IsHoliday'] if 'IsHoliday' in row else False\n            self.holiday_weights[date_key] = 5 if is_holiday else 1\n    \n    def _add_holiday_weights_to_data(self):\n        self.train_data['Weight'] = self.train_data.apply(\n            lambda row: self.holiday_weights.get((row['Store'], row['Date']), 1), axis=1\n        )\n        \n        self.test_data['Weight'] = self.test_data.apply(\n            lambda row: self.holiday_weights.get((row['Store'], row['Date']), 1), axis=1\n        )\n    \n    def explore_data(self):\n       \n        print(\"Data Exploration:\")\n        print(f\"Date range: {self.train_data['Date'].min()} to {self.train_data['Date'].max()}\")\n        print(f\"Number of stores: {self.train_data['Store'].nunique()}\")\n        print(f\"Number of departments: {self.train_data['Dept'].nunique()}\")\n        print(f\"Number of store-department combinations: {self.train_data['Store_Dept'].nunique()}\")\n        \n        plt.figure(figsize=(15, 8))\n        \n        weekly_total = self.train_data.groupby('Date')['Weekly_Sales'].sum().reset_index()\n        \n        plt.subplot(2, 2, 1)\n        plt.plot(weekly_total['Date'], weekly_total['Weekly_Sales'])\n        plt.title('Total Weekly Sales Over Time')\n        plt.xlabel('Date')\n        plt.ylabel('Weekly Sales')\n        plt.xticks(rotation=45)\n        \n        plt.subplot(2, 2, 2)\n        plt.hist(self.train_data['Weekly_Sales'], bins=30, alpha=0.7)\n        plt.title('Distribution of Weekly Sales')\n        plt.xlabel('Weekly Sales')\n        plt.ylabel('Frequency')\n        plt.xlim(0, 300000)\n        \n        plt.subplot(2, 2, 3)\n        store_sales = self.train_data.groupby('Store')['Weekly_Sales'].sum().sort_values(ascending=False).head(10)\n        plt.bar(range(len(store_sales)), store_sales.values)\n        plt.title('Top 10 Stores by Total Sales')\n        plt.xlabel('Store Rank')\n        plt.ylabel('Total Sales')\n        \n        plt.subplot(2, 2, 4)\n        dept_sales = self.train_data.groupby('Dept')['Weekly_Sales'].sum().sort_values(ascending=False).head(10)\n        plt.bar(range(len(dept_sales)), dept_sales.values)\n        plt.title('Top 10 Departments by Total Sales')\n        plt.xlabel('Department Rank')\n        plt.ylabel('Total Sales')\n        \n        plt.tight_layout()\n        plt.show()\n\n    \n    def plot_decomposition(self, timeseries, title):\n        plt.figure(figsize=(15, 10))\n        decomposition = seasonal_decompose(timeseries, model='additive', period=52)  # Weekly data, yearly seasonality\n        \n        plt.subplot(4, 1, 1)\n        decomposition.observed.plot(title=f'{title} - Original')\n        plt.subplot(4, 1, 2)\n        decomposition.trend.plot(title='Trend')\n        plt.subplot(4, 1, 3)\n        decomposition.seasonal.plot(title='Seasonal')\n        plt.subplot(4, 1, 4)\n        decomposition.resid.plot(title='Residual')\n        plt.tight_layout()\n        plt.show()\n        \n        return decomposition\n    \n\n   \n    def forecast_sales(self, store_dept_id, forecast_periods):\n        if store_dept_id not in self.models:\n            print(f\"No model found for {store_dept_id}\")\n            return None\n        \n        model = self.models[store_dept_id]\n        \n        forecast = model.forecast(steps=forecast_periods)\n        forecast_ci = model.get_forecast(steps=forecast_periods).conf_int()\n        \n        forecast = np.maximum(forecast, 0)\n        \n        return forecast, forecast_ci\n    \n    def generate_submission(self, submission_path='submission.csv'):\n        print(\"Generating submission file...\")\n        \n        test_combinations = self.test_data['Store_Dept'].unique()\n        \n        \n        submission = self.test_data[['Store', 'Dept', 'Date']].copy()\n        submission['Weekly_Sales'] = 0\n        \n        for store_dept in test_combinations:\n            if store_dept in self.models:\n                test_subset = self.test_data[self.test_data['Store_Dept'] == store_dept]\n                forecast_periods = len(test_subset)\n                \n                forecast, _ = self.forecast_sales(store_dept, forecast_periods)\n                \n                if forecast is not None:\n                    mask = submission['Store_Dept'] == store_dept\n                    submission.loc[mask, 'Weekly_Sales'] = forecast\n            else:\n                store = int(store_dept.split('_')[0])\n                dept = int(store_dept.split('_')[1])\n                \n                historical_mean = (self.train_data[\n                    (self.train_data['Store'] == store) & \n                    (self.train_data['Dept'] == dept)\n                ]['Weekly_Sales'].mean())\n                \n                if pd.isna(historical_mean):\n                    historical_mean = self.train_data['Weekly_Sales'].mean()\n                \n                mask = (submission['Store'] == store) & (submission['Dept'] == dept)\n                submission.loc[mask, 'Weekly_Sales'] = historical_mean\n        \n        submission['Store_Dept'] = submission['Store'].astype(str) + '_' + submission['Dept'].astype(str)\n        \n        submission[['Store', 'Dept', 'Date', 'Weekly_Sales']].to_csv(submission_path, index=False)\n        print(f\"Submission saved to {submission_path}\")\n        \n        return submission\n    \n    def evaluate_model(self, test_size=0.2, top_n=50):\n\n        top_combinations = (self.train_data.groupby('Store_Dept')['Weekly_Sales']\n                          .sum()\n                          .sort_values(ascending=False)\n                          .head(top_n)\n                          .index.tolist())\n\n\n        print(f\"combs - {top_combinations}\")\n        evaluations = {}\n        for i, store_dept_id in enumerate(top_combinations):\n            # print(f\"\\nTraining model {i+1}/{len(top_combinations)}: {store_dept_id}\")\n\n            if i % 10 is 0:\n                print(f\"checkpoint - {i+1}\")\n            full_data = self.train_data[self.train_data['Store_Dept'] == store_dept_id].copy()\n            full_data = full_data.set_index('Date').sort_index()\n            \n            ts_data = full_data['Weekly_Sales']\n            weights = full_data['Weight'] if 'Weight' in full_data.columns else pd.Series([1] * len(ts_data), index=ts_data.index)\n\n            split_point = int(len(ts_data) * (1 - test_size))\n            train_data = ts_data[:split_point]\n            test_data = ts_data[split_point:]\n            test_weights = weights[split_point:]\n\n            model = SARIMAX(train_data,\n                           order=(0, 1, 1),\n                           seasonal_order=(0, 1, 1, 52),\n                           enforce_stationarity=False,\n                           enforce_invertibility=False)\n\n            fitted_model = model.fit(disp=False, maxiter=30)\n            forecast = fitted_model.forecast(steps=len(test_data))\n            wmae = weighted_mean_absolute_error(test_data, forecast, test_weights)\n\n            evaluations[store_dept_id] = {'forecast': forecast, 'actual': test_data, 'weights': test_weights}\n            \n            \n        return evaluations   \n    \n    def calculate_overall_wmae(self, predictions_dict):\n        all_actual = []\n        all_predicted = []\n        all_weights = []\n        \n        for store_dept, results in predictions_dict.items():\n            if results is not None:\n                all_actual.extend(results['actual'])\n                all_predicted.extend(results['forecast'])\n                all_weights.extend(results['weights'])\n        \n        if len(all_actual) == 0:\n            return None\n        \n        overall_wmae = weighted_mean_absolute_error(all_actual, all_predicted, all_weights)\n        print(f\"Overall WMAE across all combinations: {overall_wmae:.2f}\")\n        \n        return overall_wmae","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T13:38:45.474091Z","iopub.execute_input":"2025-07-06T13:38:45.474396Z","iopub.status.idle":"2025-07-06T13:38:45.510722Z","shell.execute_reply.started":"2025-07-06T13:38:45.474375Z","shell.execute_reply":"2025-07-06T13:38:45.509584Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    forecaster = WalmartSARIMAForecaster()\n    \n    train_data, test_data = forecaster.load_and_preprocess_data(\n        train_path='/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip',\n        test_path='/kaggle/input/walmart-recruiting-store-sales-forecasting/test.csv.zip',\n        stores_path='/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv',  \n        features_path='/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv.zip'  \n    )\n    \n    \n\n    evaluations = forecaster.evaluate_model(top_n=2500)\n    overall_wmae = forecaster.calculate_overall_wmae(evaluations)\n    \n    print(f\"\\nSARIMA modeling complete!\")\n    print(f\"Trained models: {len(forecaster.models)}\")\n    print(f\"Overall WMAE: {overall_wmae:.2f}\" if overall_wmae else \"WMAE calculation failed\")\n    # print(\"Submission file generated: walmart_sarima_submission.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}